{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d844290-90b1-4f9e-900e-f4798bf86aec",
   "metadata": {},
   "source": [
    "# Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4aee5f-0732-48e2-995e-0b7907344ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fs1620/MLBD_2024_25/ResearchProject/LiaDataAnalysis_final/FeatureSelection\n",
      "Changed working directory to: /home/fs1620/MLBD_2024_25/ResearchProject/LiaDataAnalysis_final/FeatureSelection/PreppedData\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "cwd = Path.cwd()\n",
    "print(cwd)\n",
    "prepped_data_dir = cwd / \"PreppedData\"\n",
    "\n",
    "# If current path does not end in \"PreppedData\", move into it\n",
    "if cwd.name != \"PreppedData\":\n",
    "    if prepped_data_dir.exists():\n",
    "        os.chdir(prepped_data_dir)\n",
    "        print(f\"Changed working directory to: {prepped_data_dir}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"'Annotations' directory not found at: {prepped_data_dir}\")\n",
    "else:\n",
    "    print(f\"Already in {cwd.name} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f2f57-2f69-4b54-a4d5-35be52df5099",
   "metadata": {},
   "source": [
    "# Checks for discrepancies in metric computations\n",
    "\n",
    "I suspect there are some shenanigans with how `imblearn`'s `geometric_mean_score` computes results with `average='macro'` or `'weighted'`. There are some slight numerical discrepancies as can be seen below. We compare the results given by the theoretical formulae for macro and weighted G-mean:\n",
    "\n",
    "---\n",
    "\n",
    "### Macro G-mean\n",
    "\n",
    "For $K$ classes with per-class recalls (sensitivities) $r_i$:\n",
    "\n",
    "$$\\text{G-mean}_{\\text{macro}} = \\left( \\prod_{i=1}^{K} r_i \\right)^{1/K}$$\n",
    "\n",
    "Each class contributes equally, regardless of its support.\n",
    "\n",
    "---\n",
    "\n",
    "### Weighted G-mean\n",
    "\n",
    "For class $i$ with support (number of true instances) $n_i$:\n",
    "\n",
    "$$\\text{G-mean}_{\\text{weighted}} = \\left( \\prod_{i=1}^{K} r_i^{\\, n_i} \\right)^{1 / \\sum_{i=1}^{K} n_i}$$\n",
    "\n",
    "This is a **weighted geometric mean**, where classes with more samples have a larger influence on the final value.\n",
    "\n",
    "We can only guess that the discrepancies arise due to some internal numerical smoothing applied by `imblearn` (at least that's what the [source code suggests](https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/imblearn/metrics/_classification.py#L548))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8132508-1fcc-4db6-8331-bc894ab1659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results loaded.\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 895\n",
      "Class 2: 436\n",
      "\n",
      "\n",
      "--- Per-class Sensitivities (G-mean none) ---\n",
      "[0.86153508 0.84048379 0.91152513]\n",
      "\n",
      "\n",
      "--- G-mean Macro ---\n",
      "Manual macro G-mean (matches imblearn): 0.870676824713551\n",
      "Imbalanced-learn macro G-mean: 0.8741303181152683\n",
      "\n",
      "\n",
      "--- G-mean Weighted ---\n",
      "Manual weighted G-mean (matches imblearn): 0.862224378826243\n",
      "Imbalanced-learn weighted G-mean: 0.8659670467311678\n"
     ]
    }
   ],
   "source": [
    "# --- Multiclass case ---\n",
    "\n",
    "# --- Load CV results ---\n",
    "config = 'config2'\n",
    "model_name = 'lda'\n",
    "model_results = joblib.load(Path.cwd()/f'ResultsCV_{config}'/f'{model_name}_results_{config}.pkl')\n",
    "all_y_true, all_y_pred = model_results['all_y_true'], model_results['all_y_pred']\n",
    "print('CV results loaded.')\n",
    "\n",
    "# --- Unique classes and counts ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Per-class G-mean using imblearn (smoothed internally) ---\n",
    "gmean_none = geometric_mean_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"--- Per-class Sensitivities (G-mean none) ---\")\n",
    "print(gmean_none)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro and weighted G-mean using imblearn ---\n",
    "gmean_macro = geometric_mean_score(all_y_true, all_y_pred, average='macro')\n",
    "gmean_weighted = geometric_mean_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# --- Manual calculation matching imblearn ---\n",
    "# Macro G-mean: geometric mean of per-class sensitivities (from gmean_none)\n",
    "manual_macro = gmean_none.prod()**(1/len(gmean_none))\n",
    "\n",
    "# Weighted G-mean: weighted geometric mean using class supports\n",
    "weights = counts / counts.sum()\n",
    "manual_weighted = np.prod(gmean_none ** weights)\n",
    "\n",
    "print(\"--- G-mean Macro ---\")\n",
    "print(\"Manual macro G-mean (matches imblearn):\", manual_macro)\n",
    "print(\"Imbalanced-learn macro G-mean:\", gmean_macro)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- G-mean Weighted ---\")\n",
    "print(\"Manual weighted G-mean (matches imblearn):\", manual_weighted)\n",
    "print(\"Imbalanced-learn weighted G-mean:\", gmean_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b918f82-f548-49f0-bef5-e734d90b80a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results loaded.\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1637\n",
      "Class 1: 895\n",
      "Class 2: 363\n",
      "Class 3: 73\n",
      "Class 4: 97\n",
      "\n",
      "\n",
      "--- Per-class Sensitivities (G-mean none) ---\n",
      "[0.86811752 0.85238776 0.9396231  0.94061502 0.94032714]\n",
      "\n",
      "\n",
      "--- G-mean Macro ---\n",
      "Manual macro G-mean (matches imblearn): 0.9073441789132661\n",
      "Imbalanced-learn macro G-mean: 0.9091047626813133\n",
      "\n",
      "\n",
      "--- G-mean Weighted ---\n",
      "Manual weighted G-mean (matches imblearn): 0.875504965616811\n",
      "Imbalanced-learn weighted G-mean: 0.8777141804894034\n"
     ]
    }
   ],
   "source": [
    "# --- Multiclass case ---\n",
    "\n",
    "# --- Load CV results ---\n",
    "config = 'config3'\n",
    "model_name = 'lda'\n",
    "model_results = joblib.load(Path.cwd()/f'ResultsCV_{config}'/f'{model_name}_results_{config}.pkl')\n",
    "all_y_true, all_y_pred = model_results['all_y_true'], model_results['all_y_pred']\n",
    "print('CV results loaded.')\n",
    "\n",
    "# --- Unique classes and counts ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Per-class G-mean using imblearn (smoothed internally) ---\n",
    "gmean_none = geometric_mean_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"--- Per-class Sensitivities (G-mean none) ---\")\n",
    "print(gmean_none)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro and weighted G-mean using imblearn ---\n",
    "gmean_macro = geometric_mean_score(all_y_true, all_y_pred, average='macro')\n",
    "gmean_weighted = geometric_mean_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# --- Manual calculation matching imblearn ---\n",
    "# Macro G-mean: geometric mean of per-class sensitivities (from gmean_none)\n",
    "manual_macro = gmean_none.prod()**(1/len(gmean_none))\n",
    "\n",
    "# Weighted G-mean: weighted geometric mean using class supports\n",
    "weights = counts / counts.sum()\n",
    "manual_weighted = np.prod(gmean_none ** weights)\n",
    "\n",
    "print(\"--- G-mean Macro ---\")\n",
    "print(\"Manual macro G-mean (matches imblearn):\", manual_macro)\n",
    "print(\"Imbalanced-learn macro G-mean:\", gmean_macro)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- G-mean Weighted ---\")\n",
    "print(\"Manual weighted G-mean (matches imblearn):\", manual_weighted)\n",
    "print(\"Imbalanced-learn weighted G-mean:\", gmean_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67cddac-f2ee-4d67-b335-03fa3dba01bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results loaded.\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 1331\n",
      "\n",
      "\n",
      "--- Per-class Sensitivities (G-mean none) ---\n",
      "[0.86436415 0.86436415]\n",
      "\n",
      "\n",
      "--- G-mean Macro ---\n",
      "Manual macro G-mean (matches imblearn): 0.8643641454747898\n",
      "Imbalanced-learn macro G-mean: 0.8652696284241367\n",
      "\n",
      "\n",
      "--- G-mean Weighted ---\n",
      "Manual weighted G-mean (matches imblearn): 0.8643641454747899\n",
      "Imbalanced-learn weighted G-mean: 0.865253982326384\n"
     ]
    }
   ],
   "source": [
    "# --- Binary case ---\n",
    "\n",
    "# --- Load CV results ---\n",
    "config = 'config1'\n",
    "model_name = 'lda'\n",
    "model_results = joblib.load(Path.cwd()/f'ResultsCV_{config}'/f'{model_name}_results_{config}.pkl')\n",
    "all_y_true, all_y_pred = model_results['all_y_true'], model_results['all_y_pred']\n",
    "print('CV results loaded.')\n",
    "\n",
    "# --- Unique classes and counts ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Per-class G-mean using imblearn (smoothed internally) ---\n",
    "gmean_none = geometric_mean_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"--- Per-class Sensitivities (G-mean none) ---\")\n",
    "print(gmean_none)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro and weighted G-mean using imblearn ---\n",
    "gmean_macro = geometric_mean_score(all_y_true, all_y_pred, average='macro')\n",
    "gmean_weighted = geometric_mean_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# --- Manual calculation matching imblearn ---\n",
    "# Macro G-mean: geometric mean of per-class sensitivities (from gmean_none)\n",
    "manual_macro = gmean_none.prod()**(1/len(gmean_none))\n",
    "\n",
    "# Weighted G-mean: weighted geometric mean using class supports\n",
    "weights = counts / counts.sum()\n",
    "manual_weighted = np.prod(gmean_none ** weights)\n",
    "\n",
    "print(\"--- G-mean Macro ---\")\n",
    "print(\"Manual macro G-mean (matches imblearn):\", manual_macro)\n",
    "print(\"Imbalanced-learn macro G-mean:\", gmean_macro)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- G-mean Weighted ---\")\n",
    "print(\"Manual weighted G-mean (matches imblearn):\", manual_weighted)\n",
    "print(\"Imbalanced-learn weighted G-mean:\", gmean_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2f92c5-10a9-4e69-a856-2ea4bbf7d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-class Sensitivity (Recall) ---\n",
      "Class 0: 0.9268\n",
      "Class 1: 0.7441\n",
      "Class 2: 0.8463\n",
      "\n",
      "\n",
      "--- Macro Sensitivity ---\n",
      "Macro sensitivity from sklearn: 0.839074430770438\n",
      "Macro sensitivity (average of per-class recalls): 0.839074430770438\n",
      "\n",
      "\n",
      "--- Weighted Sensitivity ---\n",
      "Weighted sensitivity: 0.8619902120717782\n",
      "Weighted sensitivity: 0.8619902120717782\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 895\n",
      "Class 2: 436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# --- Per-class sensitivity (recall) ---\n",
    "per_class_sens = recall_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"-\"*3 + \" Per-class Sensitivity (Recall) \" + \"-\"*3)\n",
    "for i, sens in enumerate(per_class_sens):\n",
    "    print(f\"Class {i}: {sens:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro sensitivity (unweighted mean of per-class recalls) ---\n",
    "macro_sens_sk =  recall_score(all_y_true, all_y_pred, average='macro')\n",
    "macro_sens = per_class_sens.mean()\n",
    "print(\"-\"*3 + \" Macro Sensitivity \" + \"-\"*3)\n",
    "print(\"Macro sensitivity from sklearn:\", macro_sens_sk)\n",
    "print(\"Macro sensitivity (average of per-class recalls):\", macro_sens)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Weighted sensitivity (weighted mean by class support) ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "weighted_sens = np.sum(per_class_sens * counts) / counts.sum()\n",
    "weighted_sens_sk =  recall_score(all_y_true, all_y_pred, average='weighted')\n",
    "print(\"-\"*3 + \" Weighted Sensitivity \" + \"-\"*3)\n",
    "print(\"Weighted sensitivity:\", weighted_sens)\n",
    "print(\"Weighted sensitivity:\", weighted_sens_sk)\n",
    "\n",
    "# --- Optional: per-class support counts ---\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e81183d-8b50-49b4-a14a-b3bc7264c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-class Precision ---\n",
      "Class 0: 0.8584\n",
      "Class 1: 0.8582\n",
      "Class 2: 0.8849\n",
      "\n",
      "\n",
      "--- Macro Precision ---\n",
      "Macro precision from sklearn: 0.8671932266505061\n",
      "Macro precision (average of per-class precision): 0.8671932266505061\n",
      "\n",
      "\n",
      "--- Weighted Precision ---\n",
      "Weighted precision (manual): 0.8621467045186014\n",
      "Weighted precision from sklearn: 0.8621467045186014\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 895\n",
      "Class 2: 436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# --- Per-class precision ---\n",
    "per_class_prec = precision_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"-\"*3 + \" Per-class Precision \" + \"-\"*3)\n",
    "for i, prec in enumerate(per_class_prec):\n",
    "    print(f\"Class {i}: {prec:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro precision (unweighted mean of per-class precision) ---\n",
    "macro_prec_sk = precision_score(all_y_true, all_y_pred, average='macro')\n",
    "macro_prec = per_class_prec.mean()\n",
    "print(\"-\"*3 + \" Macro Precision \" + \"-\"*3)\n",
    "print(\"Macro precision from sklearn:\", macro_prec_sk)\n",
    "print(\"Macro precision (average of per-class precision):\", macro_prec)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Weighted precision (weighted mean by class support) ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "weighted_prec = np.sum(per_class_prec * counts) / counts.sum()\n",
    "weighted_prec_sk = precision_score(all_y_true, all_y_pred, average='weighted')\n",
    "print(\"-\"*3 + \" Weighted Precision \" + \"-\"*3)\n",
    "print(\"Weighted precision (manual):\", weighted_prec)\n",
    "print(\"Weighted precision from sklearn:\", weighted_prec_sk)\n",
    "\n",
    "# --- Optional: per-class support counts ---\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d0e7d6-c991-4f89-9d9b-e5d96f7899b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-class Specificity ---\n",
      "Class 0: 0.8009\n",
      "Class 1: 0.9493\n",
      "Class 2: 0.9817\n",
      "\n",
      "\n",
      "--- Macro Specificity ---\n",
      "Macro specificity (average of per-class specificity): 0.9106508135955235\n",
      "\n",
      "\n",
      "--- Weighted Specificity ---\n",
      "Weighted specificity: 0.8699622287147921\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 895\n",
      "Class 2: 436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- Compute per-class specificity ---\n",
    "classes = np.unique(all_y_true)\n",
    "per_class_spec = []\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred, labels=classes)\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    per_class_spec.append(spec)\n",
    "\n",
    "per_class_spec = np.array(per_class_spec)\n",
    "\n",
    "print(\"-\"*3 + \" Per-class Specificity \" + \"-\"*3)\n",
    "for cls, spec in zip(classes, per_class_spec):\n",
    "    print(f\"Class {cls}: {spec:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro specificity (unweighted mean of per-class specificity) ---\n",
    "macro_spec = per_class_spec.mean()\n",
    "print(\"-\"*3 + \" Macro Specificity \" + \"-\"*3)\n",
    "print(\"Macro specificity (average of per-class specificity):\", macro_spec)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Weighted specificity (weighted by class support) ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "weighted_spec = np.sum(per_class_spec * counts) / counts.sum()\n",
    "print(\"-\"*3 + \" Weighted Specificity \" + \"-\"*3)\n",
    "print(\"Weighted specificity:\", weighted_spec)\n",
    "\n",
    "# --- Optional: per-class support counts ---\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031248e2-cd98-4bff-9d27-189811e2bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-class Recalls (Sensitivity) ---\n",
      "Class 0: 0.9268\n",
      "Class 1: 0.7441\n",
      "Class 2: 0.8463\n",
      "\n",
      "\n",
      "--- Macro Balanced Accuracy ---\n",
      "Theoretical macro balanced accuracy: 0.839074430770438\n",
      "Sklearn balanced_accuracy_score (macro): 0.839074430770438\n",
      "\n",
      "\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 895\n",
      "Class 2: 436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, balanced_accuracy_score\n",
    "\n",
    "# --- Per-class recalls (sensitivities) ---\n",
    "per_class_recalls = recall_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"-\"*3 + \" Per-class Recalls (Sensitivity) \" + \"-\"*3)\n",
    "for i, r in enumerate(per_class_recalls):\n",
    "    print(f\"Class {i}: {r:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro balanced accuracy (average of per-class recalls) ---\n",
    "macro_bal_acc_theoretical = per_class_recalls.mean()\n",
    "macro_bal_acc_sklearn = balanced_accuracy_score(all_y_true, all_y_pred, adjusted=False)\n",
    "print(\"-\"*3 + \" Macro Balanced Accuracy \" + \"-\"*3)\n",
    "print(\"Theoretical macro balanced accuracy:\", macro_bal_acc_theoretical)\n",
    "print(\"Sklearn balanced_accuracy_score (macro):\", macro_bal_acc_sklearn)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Optional: per-class support counts ---\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582347b1-5625-4b97-826c-d0dfe93b3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-class F1-score ---\n",
      "Class 0: 0.8913\n",
      "Class 1: 0.7971\n",
      "Class 2: 0.8652\n",
      "\n",
      "\n",
      "--- Macro F1-score ---\n",
      "Macro F1-score from sklearn: 0.8512004902715056\n",
      "Macro F1-score (average of per-class F1): 0.8512004902715056\n",
      "\n",
      "\n",
      "--- Weighted F1-score ---\n",
      "Weighted F1-score (manual): 0.860081286325489\n",
      "Weighted F1-score from sklearn: 0.860081286325489\n",
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 895\n",
      "Class 2: 436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Per-class F1-score ---\n",
    "per_class_f1 = f1_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"-\"*3 + \" Per-class F1-score \" + \"-\"*3)\n",
    "for i, f1 in enumerate(per_class_f1):\n",
    "    print(f\"Class {i}: {f1:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro F1-score (unweighted mean of per-class F1) ---\n",
    "macro_f1_sk = f1_score(all_y_true, all_y_pred, average='macro')\n",
    "macro_f1 = per_class_f1.mean()\n",
    "print(\"-\"*3 + \" Macro F1-score \" + \"-\"*3)\n",
    "print(\"Macro F1-score from sklearn:\", macro_f1_sk)\n",
    "print(\"Macro F1-score (average of per-class F1):\", macro_f1)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Weighted F1-score (weighted mean by class support) ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "weighted_f1 = np.sum(per_class_f1 * counts) / counts.sum()\n",
    "weighted_f1_sk = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "print(\"-\"*3 + \" Weighted F1-score \" + \"-\"*3)\n",
    "print(\"Weighted F1-score (manual):\", weighted_f1)\n",
    "print(\"Weighted F1-score from sklearn:\", weighted_f1_sk)\n",
    "\n",
    "# --- Optional: per-class support counts ---\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db765f-1f1b-4253-a1df-5292fdd27cc2",
   "metadata": {},
   "source": [
    "This proves that there are some discrepancies with how the geometric mean is computed by `imblearn`, compared to what the theoretical formula would suggest. We have verified that this is only the case in the multiclass case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d268af-6ee8-41c3-9e6b-cba86bafc1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Load CV results ---\n",
    "config = 'config1'\n",
    "model_name = 'lda'\n",
    "lda_results = joblib.load(Path.cwd()/f'ResultsCV_{config}'/f'{model_name}_results_{config}.pkl')\n",
    "all_y_true, all_y_pred = lda_results['all_y_true'], lda_results['all_y_pred']\n",
    "print('CV results loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edabc424-cc45-4116-9ca9-6cccd33a21d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 1331\n",
      "\n",
      "\n",
      "--- Per-class Sensitivities ---\n",
      "G-mean with average = 'none' (per-class sensitivities): [0.86436415 0.86436415]\n",
      "\n",
      "\n",
      "--- G-mean Macro ---\n",
      "Theoretical macro G-mean using per-class recalls: 0.8643641454747898\n",
      "Imbalanced-learn's macro G-mean: 0.8652696284241367\n",
      "\n",
      "\n",
      "--- G-mean weighted ---\n",
      "Theoretical weighted G-mean using per-class recalls and supports: 0.8643641454747899\n",
      "Imbalanced-learn's weighted G-mean: 0.865253982326384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "# --- Optional: per-class support counts ---\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Per-class G-mean (no averaging) ---\n",
    "gmean_none = geometric_mean_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"-\"*3+\" Per-class Sensitivities \"+\"-\"*3)\n",
    "print(\"G-mean with average = 'none' (per-class sensitivities):\", gmean_none)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro and weighted G-mean using imbalanced-learn ---\n",
    "gmean_macro = geometric_mean_score(all_y_true, all_y_pred, average='macro')\n",
    "gmean_weighted = geometric_mean_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# --- Manual calculation of theoretical macro G-mean ---\n",
    "theoretical_macro = gmean_none.prod()**(1/len(gmean_none))\n",
    "\n",
    "# --- Manual calculation of theoretical weighted G-mean ---\n",
    "weighted_exponent = counts / counts.sum()\n",
    "theoretical_weighted = np.prod(gmean_none ** weighted_exponent)\n",
    "\n",
    "print(\"-\"*3+\" G-mean Macro \"+\"-\"*3)\n",
    "print(\"Theoretical macro G-mean using per-class recalls:\", theoretical_macro)\n",
    "print(\"Imbalanced-learn's macro G-mean:\", gmean_macro)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"-\"*3+\" G-mean weighted \"+\"-\"*3)\n",
    "print(\"Theoretical weighted G-mean using per-class recalls and supports:\", theoretical_weighted)\n",
    "print(\"Imbalanced-learn's weighted G-mean:\", gmean_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51797b5f-18a3-4db3-918f-4e709fefaead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class supports (number of true instances per class):\n",
      "Class 0: 1734\n",
      "Class 1: 1331\n",
      "\n",
      "\n",
      "--- Per-class Sensitivities ---\n",
      "G-mean with average='none' (per-class sensitivities): [0.86436415 0.86436415]\n",
      "\n",
      "\n",
      "--- G-mean Macro ---\n",
      "Theoretical macro G-mean using per-class recalls: 0.8643641454747898\n",
      "Imbalanced-learn's macro G-mean: 0.8652696284241367\n",
      "\n",
      "\n",
      "--- G-mean Weighted ---\n",
      "Theoretical weighted G-mean using per-class recalls and supports: 0.8643641454747899\n",
      "Imbalanced-learn's weighted G-mean: 0.865253982326384\n",
      "\n",
      "\n",
      "--- Exact reproduction using confusion matrix ---\n",
      "Per-class recalls: [0.90484429 0.82569497]\n",
      "Macro G-mean (exact): 0.8643641454747898\n",
      "Weighted G-mean (exact): 0.8695814668708433\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- Unique classes and counts ---\n",
    "classes, counts = np.unique(all_y_true, return_counts=True)\n",
    "\n",
    "print(\"\\nClass supports (number of true instances per class):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Per-class G-mean using imblearn ---\n",
    "gmean_none = geometric_mean_score(all_y_true, all_y_pred, average=None)\n",
    "print(\"--- Per-class Sensitivities ---\")\n",
    "print(\"G-mean with average='none' (per-class sensitivities):\", gmean_none)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Macro and weighted G-mean using imblearn ---\n",
    "gmean_macro = geometric_mean_score(all_y_true, all_y_pred, average='macro')\n",
    "gmean_weighted = geometric_mean_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# --- Manual theoretical calculation using per-class recalls from imblearn ---\n",
    "# Macro G-mean (geometric mean of per-class recalls)\n",
    "theoretical_macro = gmean_none.prod()**(1/len(gmean_none))\n",
    "\n",
    "# Weighted G-mean (weighted geometric mean using support counts)\n",
    "weighted_exponent = counts / counts.sum()\n",
    "theoretical_weighted = np.prod(gmean_none ** weighted_exponent)\n",
    "\n",
    "print(\"--- G-mean Macro ---\")\n",
    "print(\"Theoretical macro G-mean using per-class recalls:\", theoretical_macro)\n",
    "print(\"Imbalanced-learn's macro G-mean:\", gmean_macro)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- G-mean Weighted ---\")\n",
    "print(\"Theoretical weighted G-mean using per-class recalls and supports:\", theoretical_weighted)\n",
    "print(\"Imbalanced-learn's weighted G-mean:\", gmean_weighted)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Exact reproduction using confusion matrix (matches imblearn exactly) ---\n",
    "cm = confusion_matrix(all_y_true, all_y_pred, labels=classes)\n",
    "per_class_recall = np.array([cm[i,i]/cm[i].sum() if cm[i].sum() > 0 else np.nan \n",
    "                             for i in range(len(classes))])\n",
    "\n",
    "# Macro G-mean\n",
    "macro_gmean_exact = np.nanprod(per_class_recall)**(1/np.sum(~np.isnan(per_class_recall)))\n",
    "# Weighted G-mean\n",
    "weighted_gmean_exact = np.prod(per_class_recall ** (counts/counts.sum()))\n",
    "\n",
    "print(\"--- Exact reproduction using confusion matrix ---\")\n",
    "print(\"Per-class recalls:\", per_class_recall)\n",
    "print(\"Macro G-mean (exact):\", macro_gmean_exact)\n",
    "print(\"Weighted G-mean (exact):\", weighted_gmean_exact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7b39a-e5da-4f61-9eb5-d0a0f0df8e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14aba70-8e81-4f5b-9c85-79d5bedf3eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
