{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f10a39-cc73-4606-b421-bd43018b2e1e",
   "metadata": {},
   "source": [
    "# Basic Imports and Data + Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ab72a4-6999-4f91-8f57-158d8a4d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# Key imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Preprocessing\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ML implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Model evaluation\n",
    "import sklearn.metrics as skmetrics\n",
    "import imblearn.metrics as imbmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23d3a8f-78d4-4cd9-9a2b-99b50496ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44553aa4-264d-4808-b919-224f4d4e118d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root directory: /home/fs1620/MLBD_2024_25/Research_Project/LiaDataAnalysis/SampleSizePowerCalc\n",
      "Loaded data from /home/fs1620/MLBD_2024_25/Research_Project/LiaDataAnalysis/SampleSizePowerCalc/PreppedData_config1/combined_log_transformed_config1.npz\n",
      "X:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        7.47222030e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        8.29809237e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class counts: Counter({np.int64(0): 1734, np.int64(1): 1331})\n",
      "\n",
      "X shape: (3065, 2947) y shape: (3065,)\n",
      "\n",
      "Unique class labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Set configuration\n",
    "config = \"config1\"\n",
    "\n",
    "# Set project root (where PreppedData and TunedModels live)\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "# Define data and model directories\n",
    "prepped_data_dir = project_root / f\"PreppedData_{config}\"\n",
    "model_dir = project_root / f\"TunedModels_{config}\"\n",
    "bootstrap_results_dir = project_root/f\"BootstrapResults_{config}\"\n",
    "\n",
    "# Validate directories exist\n",
    "if not prepped_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Prepped data directory not found at: {prepped_data_dir}\")\n",
    "if not model_dir.exists():\n",
    "    raise FileNotFoundError(f\"Model directory not found at: {model_dir}\")\n",
    "\n",
    "# Load data\n",
    "data_file = prepped_data_dir / f'combined_log_transformed_{config}.npz'\n",
    "data = np.load(data_file)\n",
    "print(f\"Loaded data from {data_file}\")\n",
    "\n",
    "# Display relevant summary stats\n",
    "n_split = 5\n",
    "shuffle = True\n",
    "random_state = 42\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print('X:')\n",
    "display(X)\n",
    "print('\\nClass counts:', Counter(y))\n",
    "print('\\nX shape:', X.shape, 'y shape:', y.shape)\n",
    "print('\\nUnique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c33152-dd68-4f43-a9ab-29309305320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, precision_score,\n",
    "    f1_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "def evaluate_metric(y_true, y_pred, y_prob, metric='accuracy', pos_label=1):\n",
    "    \"\"\"\n",
    "    Compute performance metric based on predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like of true class labels\n",
    "    - y_pred: array-like of predicted class labels\n",
    "    - y_prob: array-like of predicted positive class probabilities\n",
    "    - metric: performance metric string\n",
    "    - pos_label: label of positive class (default=1)\n",
    "\n",
    "    Returns:\n",
    "    - metric_value: float\n",
    "    \"\"\"\n",
    "    accepted_metrics = [\n",
    "        'accuracy', 'sensitivity', 'recall', 'precision',\n",
    "        'specificity', 'f1_score', 'roc_auc', 'average_precision', 'gmean'\n",
    "    ]\n",
    "    assert metric in accepted_metrics, f\"metric must be one of {accepted_metrics}\"\n",
    "\n",
    "    if metric == 'accuracy':\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "        \n",
    "    elif metric in ['sensitivity', 'recall']:\n",
    "        return recall_score(y_true, y_pred, pos_label=pos_label)\n",
    "        \n",
    "    elif metric == 'precision':\n",
    "        return precision_score(y_true, y_pred, pos_label=pos_label)\n",
    "        \n",
    "    elif metric == 'specificity':\n",
    "        # True negatives / (true negatives + false positives)\n",
    "        tn = np.sum((y_true != pos_label) & (y_pred != pos_label))\n",
    "        fp = np.sum((y_true != pos_label) & (y_pred == pos_label))\n",
    "        return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        \n",
    "    elif metric == 'f1_score':\n",
    "        return f1_score(y_true, y_pred, pos_label=pos_label)\n",
    "        \n",
    "    elif metric == 'roc_auc':\n",
    "        return roc_auc_score(y_true, y_prob)\n",
    "        \n",
    "    elif metric == 'average_precision':\n",
    "        return average_precision_score(y_true, y_prob)\n",
    "        \n",
    "    elif metric == 'gmean':\n",
    "        return geometric_mean_score(y_true, y_pred, pos_label=pos_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4939b7-5682-429b-8208-60e93504ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def cv_bootstrap_scores(\n",
    "    X, y, model, metrics,\n",
    "    sample_fracs=np.linspace(0.1, 1.0, 10),\n",
    "    n_splits=5, n_bootstrap=100,\n",
    "    random_state=42, n_jobs=-1,\n",
    "    replace=False,\n",
    "    save_results=False,\n",
    "    save_path=\"results.pkl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns bootstrap distributions of CV scores per training fraction.\n",
    "    results[frac][metric] = {'scores': [...], 'mean': float, 'std': float, 'ci95': (lo, hi)}\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    results = {}\n",
    "\n",
    "    def run_bootstrap(seed, n_samples_inner):\n",
    "        rng_local = np.random.RandomState(seed)\n",
    "        subsample_idx = rng_local.choice(len(X), n_samples_inner, replace=replace)\n",
    "        X_sub, y_sub = X[subsample_idx], y[subsample_idx]\n",
    "        \n",
    "        # Must have both classes for stratification\n",
    "        if len(np.unique(y_sub)) < 2:\n",
    "            return None\n",
    "\n",
    "        fold_scores = {m: [] for m in metrics}\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True,\n",
    "                              random_state=rng_local.randint(1e6))\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X_sub, y_sub):\n",
    "            model_clone = clone(model)\n",
    "            X_train, X_test = X_sub[train_idx], X_sub[test_idx]\n",
    "            y_train, y_test = y_sub[train_idx], y_sub[test_idx]\n",
    "\n",
    "            model_clone.fit(X_train, y_train)\n",
    "            y_pred = model_clone.predict(X_test)\n",
    "            y_prob = None\n",
    "            \n",
    "            if hasattr(model_clone, \"predict_proba\"):\n",
    "                y_prob = model_clone.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            for metric in metrics:\n",
    "                s = evaluate_metric(y_test, y_pred, y_prob, metric=metric)\n",
    "                fold_scores[metric].append(s)\n",
    "\n",
    "        out = {}\n",
    "        for metric in metrics:\n",
    "            s = np.array(fold_scores[metric], dtype=float)\n",
    "            s = s[~np.isnan(s)]\n",
    "            if s.size:\n",
    "                out[metric] = float(np.mean(s))\n",
    "        return out\n",
    "\n",
    "    for frac in sample_fracs:\n",
    "        n_samples = int(len(X) * frac)\n",
    "        print(f\"Training fraction = {frac:.2f} ≈ {n_samples} samples\")\n",
    "\n",
    "        frac_res = {metric: {'scores': []} for metric in metrics}\n",
    "        seeds = rng.randint(0, int(1e6), size=n_bootstrap)\n",
    "\n",
    "        bootstraps = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(run_bootstrap)(s, n_samples) for s in seeds\n",
    "        )\n",
    "\n",
    "        for b in bootstraps:\n",
    "            if b is None:\n",
    "                continue\n",
    "            for metric, v in b.items():\n",
    "                frac_res[metric]['scores'].append(v)\n",
    "\n",
    "        for metric in metrics:\n",
    "            s = np.array(frac_res[metric]['scores'], dtype=float)\n",
    "            if s.size == 0:\n",
    "                continue\n",
    "            lo, hi = np.percentile(s, [2.5, 97.5])\n",
    "            frac_res[metric]['mean'] = float(np.mean(s))\n",
    "            frac_res[metric]['std'] = float(np.std(s))\n",
    "            frac_res[metric]['ci95'] = (float(lo), float(hi))\n",
    "\n",
    "        results[frac] = frac_res\n",
    "\n",
    "    # ---- SAVE RESULTS ----\n",
    "    if save_results:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        joblib.dump(results, save_path)\n",
    "        print(f\"Results saved to {save_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4220c4a5-1346-4fa4-adfc-c2778420deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_learning_curves(\n",
    "    results,\n",
    "    core_metrics=None,\n",
    "    combined_metrics=None,\n",
    "    plot_ci=True,\n",
    "    ci_type='std',             # 'std' or 'ci95'\n",
    "    figsize=(14, 5),\n",
    "    title_prefix=\"Learning Curves\",\n",
    "    legend_ncol=2,\n",
    "    save_plot=False,\n",
    "    save_folder='LearningCurves',\n",
    "    save_name='LearningCurves_model.png',\n",
    "    # ---- NEW ----\n",
    "    overlay_extrapolation=False,\n",
    "    extrapolation_df=None,     # DataFrame from plan_required_samples\n",
    "    pred_fns=None,              # dict returned from plan_required_samples\n",
    "    total_n_current=None,\n",
    "    show_ci=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot learning curves with confidence bands (std or ci95),\n",
    "    and optionally overlay extrapolated curves and target lines from plan_required_samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Canonical metric mapping ----\n",
    "    canon_map = {\n",
    "        'recall': 'recall', 'sensitivity': 'recall', 'tpr': 'recall',\n",
    "        'precision': 'precision',\n",
    "        'specificity': 'specificity', 'tnr': 'specificity',\n",
    "        'specificity_score': 'specificity', 'true_negative_rate': 'specificity',\n",
    "        'f1': 'f1_score', 'f1_score': 'f1_score',\n",
    "        'gmean': 'gmean', 'g_mean': 'gmean', 'g-mean': 'gmean',\n",
    "        'geometric_mean': 'gmean', 'geometric_mean_score': 'gmean',\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision', 'ap': 'average_precision',\n",
    "        'accuracy': 'accuracy'\n",
    "    }\n",
    "\n",
    "    pretty_labels = {\n",
    "        'recall': 'Sensitivity (Recall)',\n",
    "        'precision': 'Precision',\n",
    "        'specificity': 'Specificity',\n",
    "        'f1_score': 'F1 Score',\n",
    "        'gmean': 'G-mean',\n",
    "        'roc_auc': 'ROC-AUC',\n",
    "        'average_precision': 'Average Precision',\n",
    "        'accuracy': 'Accuracy'\n",
    "    }\n",
    "\n",
    "    # Defaults\n",
    "    if core_metrics is None:\n",
    "        core_metrics = ['recall', 'precision', 'specificity']\n",
    "    if combined_metrics is None:\n",
    "        combined_metrics = ['f1_score', 'gmean']\n",
    "\n",
    "    fracs = sorted(results.keys())\n",
    "\n",
    "    # Canonicalize results\n",
    "    canon_results = {}\n",
    "    for f in fracs:\n",
    "        cf = {}\n",
    "        for k, v in results[f].items():\n",
    "            c = canon_map.get(k, k)\n",
    "            if c in cf:\n",
    "                # merge duplicates\n",
    "                old = cf[c]\n",
    "                merged = {**old, **v}\n",
    "                if isinstance(old.get('scores', []), list) and isinstance(v.get('scores', []), list):\n",
    "                    merged['scores'] = old.get('scores', []) + v.get('scores', [])\n",
    "                cf[c] = merged\n",
    "            else:\n",
    "                cf[c] = v\n",
    "        canon_results[f] = cf\n",
    "\n",
    "    # Helper for filtering metrics\n",
    "    def available(metric):\n",
    "        return any(metric in canon_results[f] for f in fracs)\n",
    "\n",
    "    def canonical_and_filter(metric_list):\n",
    "        seen, keep = set(), []\n",
    "        for m in metric_list:\n",
    "            cm = canon_map.get(m, m)\n",
    "            if available(cm) and cm not in seen:\n",
    "                keep.append(cm)\n",
    "                seen.add(cm)\n",
    "        return keep\n",
    "\n",
    "    core_metrics = canonical_and_filter(core_metrics)\n",
    "    combined_metrics = canonical_and_filter(combined_metrics)\n",
    "\n",
    "    sets = []\n",
    "    if core_metrics:\n",
    "        sets.append(('Core metrics', core_metrics))\n",
    "    if combined_metrics:\n",
    "        sets.append(('Combined metrics', combined_metrics))\n",
    "\n",
    "    if not sets:\n",
    "        raise ValueError(\"No requested metrics were found in results.\")\n",
    "\n",
    "    # ---- Helper: build arrays ----\n",
    "    def series_for_metric(metric):\n",
    "        means, lows, highs = [], [], []\n",
    "        for f in fracs:\n",
    "            if metric not in canon_results[f]:\n",
    "                means.append(np.nan); lows.append(np.nan); highs.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            d = canon_results[f][metric]\n",
    "            mean = d.get('mean', np.nan)\n",
    "\n",
    "            if plot_ci:\n",
    "                if ci_type == 'ci95':\n",
    "                    ci = d.get('ci95', None)\n",
    "                    if ci and isinstance(ci, (list, tuple)) and len(ci) == 2 and np.all(np.isfinite(ci)):\n",
    "                        lo, hi = ci\n",
    "                    else:\n",
    "                        lo, hi = np.nan, np.nan\n",
    "                elif ci_type == 'std':\n",
    "                    std = d.get('std', np.nan)\n",
    "                    if np.isfinite(std):\n",
    "                        lo, hi = mean - std, mean + std\n",
    "                    else:\n",
    "                        lo, hi = np.nan, np.nan\n",
    "                else:\n",
    "                    raise ValueError(\"ci_type must be 'std' or 'ci95'\")\n",
    "            else:\n",
    "                lo, hi = np.nan, np.nan\n",
    "\n",
    "            means.append(mean); lows.append(lo); highs.append(hi)\n",
    "\n",
    "        return np.array(means, float), np.array(lows, float), np.array(highs, float)\n",
    "\n",
    "    # ---- Plot ----\n",
    "    ncols = len(sets)\n",
    "    fig, axs = plt.subplots(1, ncols, figsize=figsize, sharex=True)\n",
    "    if ncols == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Observed sample sizes\n",
    "    n_obs = np.array(fracs) * (total_n_current if total_n_current else 1)\n",
    "\n",
    "    for ax, (title, metric_list) in zip(axs, sets):\n",
    "        any_plotted = False\n",
    "        for metric in metric_list:\n",
    "            means, lows, highs = series_for_metric(metric)\n",
    "            if not np.isfinite(means).any():\n",
    "                continue\n",
    "\n",
    "            # Plot observed means\n",
    "            (line,) = ax.plot(n_obs, means, marker='o', label=pretty_labels.get(metric, metric))\n",
    "            any_plotted = True\n",
    "\n",
    "            # Plot CI band\n",
    "            if plot_ci:\n",
    "                mask = np.isfinite(lows) & np.isfinite(highs)\n",
    "                if mask.any():\n",
    "                    ax.fill_between(n_obs[mask], lows[mask], highs[mask],\n",
    "                                    alpha=0.2, color=line.get_color())\n",
    "\n",
    "            # ---- Overlay extrapolated curves and targets ----\n",
    "            if overlay_extrapolation and extrapolation_df is not None and pred_fns is not None:\n",
    "                row = extrapolation_df[extrapolation_df[\"metric\"] == metric]\n",
    "                if len(row):\n",
    "                    row = row.iloc[0]\n",
    "                    pred_fn = pred_fns.get(metric, None)\n",
    "                    if pred_fn:\n",
    "                        # Extrapolation up to max(n_required) or 2x current\n",
    "                        max_n = max(row.get(\"n_required\", n_obs[-1]), n_obs[-1]) * 1.2\n",
    "                        n_grid = np.linspace(1, max_n, 300)\n",
    "                        ax.plot(n_grid, pred_fn(n_grid), ls=\"--\", lw=1.2,\n",
    "                                color=line.get_color(), alpha=0.8)\n",
    "\n",
    "                    # Target horizontal line\n",
    "                    ax.axhline(row[\"target\"], ls=\"--\", color=\"red\", lw=1.3, alpha=0.7)\n",
    "\n",
    "                    # Required sample vertical line + CI band\n",
    "                    if \"n_required\" in row and np.isfinite(row[\"n_required\"]):\n",
    "                        ax.axvline(row[\"n_required\"], ls=\":\", color=\"blue\", lw=1.3, alpha=0.8,\n",
    "                                   label=f\"N required ≈ {int(row['n_required'])}\")\n",
    "                    if show_ci and \"ci95_n_lo\" in row and \"ci95_n_hi\" in row:\n",
    "                        if np.isfinite(row[\"ci95_n_lo\"]) and np.isfinite(row[\"ci95_n_hi\"]):\n",
    "                            ax.axvspan(row[\"ci95_n_lo\"], row[\"ci95_n_hi\"], color=\"blue\", alpha=0.1)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Sample size (N)\" if total_n_current else \"Training Fraction\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        if any_plotted:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            if plot_ci:\n",
    "                ci_label = \"±1 Std. Dev.\" if ci_type == \"std\" else \"95% Bootstrap CI\"\n",
    "                handles.append(Patch(facecolor='gray', alpha=0.2, label=ci_label))\n",
    "                labels.append(ci_label)\n",
    "            ax.legend(handles, labels, ncol=legend_ncol, frameon=False)\n",
    "\n",
    "    fig.suptitle(f\"{title_prefix}\", fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    if save_plot:\n",
    "        save_path = Path(save_folder)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path / save_name, dpi=300)\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67247d76-df3d-4f3c-8988-48870669e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import curve_fit, brentq\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# ------ Models for learning curves ------\n",
    "def inv_power(n, a, b, c):\n",
    "    # y = a - b * n^{-c}; a in [0,1], b>=0, c>0   (asymptote at a)\n",
    "    n = np.asarray(n, float)\n",
    "    return a - b * np.power(np.maximum(n, 1.0), -np.maximum(c, 1e-6))\n",
    "\n",
    "def michaelis_menten(n, a, b, c):\n",
    "    # y = c + a * n / (b + n); asymptote at c + a\n",
    "    n = np.asarray(n, float)\n",
    "    return c + a * (n / (np.maximum(b, 1e-6) + n))\n",
    "\n",
    "# Preferred this model, from experimentation\n",
    "def logit_logn(n, a, b):\n",
    "    # logit(y) = a + b*log(n) -> keeps y in (0,1)\n",
    "    n = np.asarray(n, float)\n",
    "    return expit(a + b * np.log(np.maximum(n, 1.0)))\n",
    "\n",
    "MODEL_SPECS = {\n",
    "    \"inv_power\": {\n",
    "        \"func\": inv_power,\n",
    "        \"p0\":  lambda n, y: (np.nanmax(y), max(np.nanmax(y)-np.nanmin(y), 1e-3), 0.5),\n",
    "        \"bounds\": ([0.0, 0.0, 1e-3], [1.0, 1.0, 5.0]), # lower and upper bounds for (a, b, c) respectively\n",
    "        \"bounded_01\": True,\n",
    "    },\n",
    "    \"mm\": {\n",
    "        \"func\": michaelis_menten,\n",
    "        \"p0\":  lambda n, y: (max(np.nanmax(y)-np.nanmin(y), 1e-3), np.nanmedian(n), np.nanmin(y)),\n",
    "        \"bounds\": ([0.0, 1e-6, 0.0], [1.0, 1e9, 1.0]), # lower and upper bounds for (a, b, c) respectively\n",
    "        \"bounded_01\": True,\n",
    "    },\n",
    "    \"logit_logn\": {\n",
    "        \"func\": logit_logn,\n",
    "        \"p0\":  lambda n, y: (logit(np.clip(np.nanmedian(y), 1e-6, 1-1e-6)), 0.2),\n",
    "        \"bounds\": ([-20.0, -5.0], [20.0, 5.0]), # lower and upper bounds for (a, b) respectively\n",
    "        \"bounded_01\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95090b2c-3ddb-4b0c-9496-6e2823c236bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Extract series from results ------\n",
    "def extract_series(results: Dict, metric: str):\n",
    "    \"\"\"\n",
    "    Extract a metric's data across all training fractions.\n",
    "\n",
    "    Returns:\n",
    "        fracs: sorted array of training fractions\n",
    "        means: mean score per fraction\n",
    "        stds: standard deviation per fraction\n",
    "        ci_lo, ci_hi: 95% confidence interval bounds per fraction\n",
    "        score_lists: list of raw bootstrap scores per fraction\n",
    "    \"\"\"\n",
    "    fracs = np.array(sorted(results.keys()), float)\n",
    "    means, stds, ci_lo, ci_hi, score_lists = [], [], [], [], []\n",
    "\n",
    "    for f in fracs:\n",
    "        d = results[f].get(metric, {})\n",
    "        means.append(d.get(\"mean\", np.nan))   # mean score\n",
    "        stds.append(d.get(\"std\", np.nan))     # std of score\n",
    "\n",
    "        # extract 95% CI if available, else NaNs\n",
    "        if \"ci95\" in d and d[\"ci95\"] is not None:\n",
    "            lo, hi = d[\"ci95\"]\n",
    "        else:\n",
    "            lo, hi = np.nan, np.nan\n",
    "        ci_lo.append(lo)\n",
    "        ci_hi.append(hi)\n",
    "\n",
    "        # bootstrap sample scores (may be empty if not recorded)\n",
    "        score_lists.append(d.get(\"scores\", []))\n",
    "\n",
    "    return (\n",
    "        fracs,\n",
    "        np.array(means, float),\n",
    "        np.array(stds, float),\n",
    "        np.array(ci_lo, float),\n",
    "        np.array(ci_hi, float),\n",
    "        score_lists,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c366a02-c4c3-41c6-99c0-ead4afec0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FitResult:\n",
    "    model: str\n",
    "    params: np.ndarray\n",
    "    pred_fn: Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "def fit_curve(n_obs: np.ndarray,\n",
    "              y_obs: np.ndarray,\n",
    "              model: str = \"inv_power\",\n",
    "              sigma: Optional[np.ndarray] = None) -> FitResult:\n",
    "    \"\"\"\n",
    "    Fit a specified learning-curve model to observed data.\n",
    "\n",
    "    Args:\n",
    "        n_obs: array of training sample sizes (x-values)\n",
    "        y_obs: array of observed metric values (y-values)\n",
    "        model: name of model in MODEL_SPECS\n",
    "        sigma: optional per-point standard deviations (used as weights)\n",
    "\n",
    "    Returns:\n",
    "        FitResult with best-fit parameters and a callable predictor.\n",
    "    \"\"\"\n",
    "    # Look up the model's function, initial guess, and bounds\n",
    "    spec = MODEL_SPECS[model]\n",
    "    func = spec[\"func\"]\n",
    "\n",
    "    # Keep only finite points (drop NaN or inf)\n",
    "    mask = np.isfinite(n_obs) & np.isfinite(y_obs)\n",
    "    n_fit, y_fit = n_obs[mask], y_obs[mask]\n",
    "\n",
    "    # Ensure we have enough points to estimate all parameters\n",
    "    n_params = len(np.atleast_1d(spec[\"p0\"](n_fit, y_fit)))\n",
    "    if n_fit.size < n_params:\n",
    "        raise ValueError(\"Not enough finite points to fit the model.\")\n",
    "\n",
    "    # Clean sigma (if provided)\n",
    "    if sigma is not None:\n",
    "        sigma = np.asarray(sigma, float)[mask]\n",
    "        if not np.isfinite(sigma).any():\n",
    "            sigma = None\n",
    "        else:\n",
    "            # Replace NaNs/zeros with the median sigma\n",
    "            med = np.nanmedian(sigma[np.isfinite(sigma)])\n",
    "            sigma = np.where(np.isfinite(sigma) & (sigma > 1e-12), sigma, med)\n",
    "\n",
    "    # Fit the model using nonlinear least squares\n",
    "    popt, _ = curve_fit(\n",
    "        func,\n",
    "        n_fit,\n",
    "        y_fit,\n",
    "        p0=spec[\"p0\"](n_fit, y_fit),    # initial guess\n",
    "        bounds=spec[\"bounds\"],          # parameter bounds\n",
    "        sigma=sigma,                    # weights (if available)\n",
    "        absolute_sigma=bool(sigma is not None),\n",
    "        maxfev=20000\n",
    "    )\n",
    "\n",
    "    # Return fitted parameters and a predictor function\n",
    "    return FitResult(model=model, params=popt, pred_fn=lambda n: func(n, *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b995da-9e14-4a6b-b764-c9b620ffc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple\n",
    "from scipy.optimize import brentq\n",
    "import numpy as np\n",
    "\n",
    "def solve_required_n(pred_fn: Callable[[np.ndarray], np.ndarray],\n",
    "                     target: float,\n",
    "                     n_lower: float,\n",
    "                     n_upper: Optional[float] = None,\n",
    "                     max_expand: int = 14,\n",
    "                     debug: bool = False) -> Optional[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Find the smallest n >= n_lower such that pred_fn(n) >= target.\n",
    "\n",
    "    Expands an upper bracket exponentially (x2) and then uses brentq root-finding on:\n",
    "        g(n) = pred_fn(n) - target\n",
    "\n",
    "    Args:\n",
    "        pred_fn: callable returning predicted metric for an array of n\n",
    "        target: target metric value\n",
    "        n_lower: starting point (minimum allowable n)\n",
    "        n_upper: optional starting upper bound (if None, guesses one)\n",
    "        max_expand: maximum number of bracket expansions before stopping\n",
    "        debug: if True, prints debug info on bracket expansion\n",
    "\n",
    "    Returns:\n",
    "        (n_required, status) where:\n",
    "            n_required: smallest n meeting target, or None if unreachable\n",
    "            status: description of how the solution was found\n",
    "    \"\"\"\n",
    "    f_lower = float(pred_fn([n_lower]))\n",
    "    if f_lower >= target:\n",
    "        return float(n_lower), 'Already meets or exceeds target'\n",
    "\n",
    "    if n_upper is None:\n",
    "        n_upper = max(n_lower * 2.0, n_lower + 1.0)\n",
    "\n",
    "    for i in range(max_expand):\n",
    "        f_upper = float(pred_fn([n_upper]))\n",
    "        if debug:\n",
    "            print(f\"[debug] Bracket expansion iteration {i}: n_upper={n_upper}, pred={f_upper}\")\n",
    "        if f_upper >= target:\n",
    "            # We have bracketed the target: solve for root of g(n) = pred_fn(n) - target\n",
    "            g = lambda n: float(pred_fn([n]) - target)\n",
    "            try:\n",
    "                n_required = float(brentq(g, n_lower, n_upper, maxiter=200))\n",
    "                return n_required, 'brentq root found'\n",
    "            except ValueError:\n",
    "                # Root finder failed (flat or non-monotonic region): return conservative upper bound\n",
    "                return float(n_upper), 'brentq failed, using conservative upper bound'\n",
    "        # Expand bracket if target not yet reached\n",
    "        n_lower, n_upper = n_upper, n_upper * 2.0\n",
    "\n",
    "    # Could not reach the target even after expanding bracket\n",
    "    return None, \"Exhausted bracket expansions, no roots found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea41292-bc6e-4b5f-b452-c0986fff6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Union\n",
    "\n",
    "def compute_target(means: np.ndarray,\n",
    "                   improvement: Tuple[str, float],\n",
    "                   benchmark: Union[float, str] = \"max_observed\",\n",
    "                   clip01: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compute a target metric value based on a baseline benchmark and a specified improvement.\n",
    "\n",
    "    Args:\n",
    "        means: array of observed mean metric values (may contain NaNs).\n",
    "        improvement: tuple ('add' or 'mult', value) specifying additive or multiplicative improvement over baseline.\n",
    "        benchmark: either 'max_observed' to use max of means as baseline, or a numeric baseline value.\n",
    "        clip01: whether to clip the target value between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        A float target value representing the improved metric.\n",
    "    \"\"\"\n",
    "    # Determine baseline\n",
    "    if benchmark == \"max_observed\":\n",
    "        base = float(np.nanmax(means))\n",
    "    elif benchmark == 'first_observed':\n",
    "        base = float(np.round(means[0], 3))\n",
    "    elif isinstance(benchmark, (int, float)):\n",
    "        base = float(benchmark)\n",
    "    else:\n",
    "        raise ValueError(\"benchmark must be 'max_observed', 'first_observed', or a numeric value\")\n",
    "\n",
    "    kind, val = improvement\n",
    "    val = float(val)\n",
    "    if kind == \"add\":\n",
    "        target = base + val\n",
    "    elif kind == \"mult\":\n",
    "        target = base * val\n",
    "    else:\n",
    "        raise ValueError(\"improvement must be ('add', Δ) or ('mult', r)\")\n",
    "\n",
    "    return float(np.clip(target, 0.0, 1.0) if clip01 else target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f3fe48-3ebb-4dec-8715-920c34990ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_required_samples(\n",
    "    results: Dict,\n",
    "    metrics: List[str],\n",
    "    total_n_current: int,\n",
    "    improvement: Tuple[str, float] | Dict[str, Tuple[str, float]],\n",
    "    benchmark: float | str = \"max_observed\",\n",
    "    model: str | Dict[str, str] = \"inv_power\",\n",
    "    ci_mode: str = \"bootstrap\",\n",
    "    ci_band_type: str = \"ci95\",\n",
    "    n_bootstrap_fits: int = 400,\n",
    "    random_state: int = 42,\n",
    "    use_sigma_from_std: bool = True,\n",
    "    save_path: str | None = None,\n",
    "    expected_annotated_per_sample: int = 183\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Callable]]:\n",
    "    \"\"\"\n",
    "    Estimate the number of additional annotated samples required to achieve\n",
    "    specified improvements in model performance metrics by fitting and extrapolating\n",
    "    learning curves.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : Dict\n",
    "        Dictionary containing learning curve data (fracs, means, stds, etc.) per metric.\n",
    "    metrics : List[str]\n",
    "        List of performance metric names to evaluate (e.g., [\"sensitivity\", \"F1\"]).\n",
    "    total_n_current : int\n",
    "        Current total number of annotated data points (e.g., pixels).\n",
    "    improvement : Tuple[str, float] or Dict[str, Tuple[str, float]]\n",
    "        Improvement targets. Each can be:\n",
    "            - (\"add\", 0.02): additive increase over baseline\n",
    "            - (\"mult\", 1.02): multiplicative increase over baseline\n",
    "        Can be a single global value or specified per metric.\n",
    "    benchmark : str or float, default=\"max_observed\"\n",
    "        Baseline to improve upon. Options:\n",
    "            - \"max_observed\": best observed score\n",
    "            - \"first_observed\": score at smallest training size\n",
    "            - float: fixed baseline value\n",
    "    model : str or Dict[str, str], default=\"inv_power\"\n",
    "        Curve fitting model to use, e.g., \"inv_power\", \"log\", \"exp\".\n",
    "        Can be a single model or a dict specifying one per metric.\n",
    "    ci_mode : str, default=\"bootstrap\"\n",
    "        Method for uncertainty estimation. Currently only \"bootstrap\" is supported.\n",
    "    ci_band_type : str, default=\"ci95\"\n",
    "        Type of confidence interval band to use (e.g., \"ci95\").\n",
    "    n_bootstrap_fits : int, default=400\n",
    "        Number of bootstrap resamples to estimate uncertainty in n_required.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "    use_sigma_from_std : bool, default=True\n",
    "        Whether to use observed standard deviations when fitting curves.\n",
    "    save_path : str or None, default=None\n",
    "        If given, saves the resulting DataFrame as a CSV at the specified path.\n",
    "    expected_annotated_per_sample : int, default=183\n",
    "        Median number of annotated pixels per new sample, used to translate pixel\n",
    "        requirements into sample counts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, Dict[str, Callable]]\n",
    "        - DataFrame summarizing required additional samples per metric, with bounds.\n",
    "        - Dictionary of fitted prediction functions per metric.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rows = []\n",
    "    pred_fns = {}\n",
    "    baseline_metrics = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        fracs, means, stds, ci_lo, ci_hi, score_lists = extract_series(results, metric)\n",
    "        if not np.isfinite(means).any():\n",
    "            rows.append(dict(metric=metric, fit_status=\"no_data\"))\n",
    "            continue\n",
    "\n",
    "        # ---- Baseline ----\n",
    "        if benchmark == \"max_observed\":\n",
    "            baseline_metrics[metric] = float(np.round(np.nanmax(means), 3))\n",
    "        elif benchmark == \"first_observed\":\n",
    "            baseline_metrics[metric] = float(np.round(means[0], 3))\n",
    "        elif isinstance(benchmark, (int, float)):\n",
    "            baseline_metrics[metric] = float(np.round(benchmark, 3))\n",
    "        else:\n",
    "            raise ValueError(\"benchmark must be 'max_observed', 'first_observed', or a number\")\n",
    "\n",
    "        # ---- Improvement ----\n",
    "        metric_improvement = improvement[metric] if isinstance(improvement, dict) else improvement\n",
    "        improve_type, improve_value = metric_improvement\n",
    "\n",
    "        # ---- Map fracs -> absolute Ns ----\n",
    "        n_obs = np.maximum(1.0, np.round(fracs * total_n_current))\n",
    "        n_current_max = int(np.round(np.nanmax(n_obs)))\n",
    "\n",
    "        # ---- Target value ----\n",
    "        target = compute_target(means, metric_improvement, benchmark=benchmark, clip01=True)\n",
    "\n",
    "        # ---- Fit mean curve ----\n",
    "        sigma = stds if (use_sigma_from_std and np.isfinite(stds).any()) else None\n",
    "        try:\n",
    "            fitting_model = model[metric] if isinstance(model, dict) else model\n",
    "            fit_mean = fit_curve(n_obs, means, model=fitting_model, sigma=sigma)\n",
    "            pred_fns[metric] = fit_mean.pred_fn\n",
    "            n_req, root_status = solve_required_n(fit_mean.pred_fn, target, n_lower=float(n_current_max))\n",
    "        except Exception as e:\n",
    "            rows.append(dict(metric=metric, baseline=baseline_metrics[metric],\n",
    "                             fit_status=f\"fit_error: {e}\"))\n",
    "            continue\n",
    "\n",
    "        # ---- Bootstrap CIs ----\n",
    "        if ci_mode == \"bootstrap\":\n",
    "            n_reqs_boot = []\n",
    "            new_samples_boot = []\n",
    "\n",
    "            for _ in range(n_bootstrap_fits):\n",
    "                boot_means = []\n",
    "                for s in score_lists:\n",
    "                    s = np.asarray(s, float)\n",
    "                    boot_means.append(rng.choice(s, size=s.size, replace=True).mean()\n",
    "                                      if s.size and np.isfinite(s).any() else np.nan)\n",
    "                boot_means = np.asarray(boot_means, float)\n",
    "\n",
    "                try:\n",
    "                    fit_b = fit_curve(n_obs, boot_means, model=fitting_model, sigma=None)\n",
    "                    n_req_b, _ = solve_required_n(fit_b.pred_fn, target, n_lower=float(n_current_max))\n",
    "                    if n_req_b and np.isfinite(n_req_b):\n",
    "                        n_reqs_boot.append(float(n_req_b))\n",
    "                        new_samples_boot.append(\n",
    "                            (n_req_b - n_current_max) / expected_annotated_per_sample\n",
    "                        )\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if n_req is None:\n",
    "                row = dict(metric=metric, \n",
    "                           baseline=baseline_metrics[metric],\n",
    "                           target=target,\n",
    "                           improve_specs = metric_improvement,\n",
    "                           n_current=n_current_max,\n",
    "                           fitting_model=fitting_model,\n",
    "                           fit_status=\"unreachable_under_model\",\n",
    "                           root_status=root_status,\n",
    "                           n_required=None, delta_n=None,\n",
    "                           new_samples_req=None,\n",
    "                           new_samples_req_lo=None,\n",
    "                           new_samples_req_hi=None,\n",
    "                           ci95_n_lo=None, ci95_n_hi=None)\n",
    "            else:\n",
    "                # Compute point estimate + conservative bounds\n",
    "                samples_req = np.ceil((n_req - n_current_max) / expected_annotated_per_sample)\n",
    "                lo_samples = np.percentile(new_samples_boot, 2.5) if new_samples_boot else samples_req\n",
    "                hi_samples = np.percentile(new_samples_boot, 97.5) if new_samples_boot else samples_req\n",
    "                lo_samples = min(samples_req, lo_samples)\n",
    "                hi_samples = max(samples_req, hi_samples)\n",
    "\n",
    "                ci_lo_n = np.percentile(n_reqs_boot, 2.5) if n_reqs_boot else n_req\n",
    "                ci_hi_n = np.percentile(n_reqs_boot, 97.5) if n_reqs_boot else n_req\n",
    "\n",
    "                row = dict(\n",
    "                    metric=metric,\n",
    "                    baseline=baseline_metrics[metric],\n",
    "                    target=target,\n",
    "                    improve_specs = metric_improvement,\n",
    "                    n_current=n_current_max,\n",
    "                    fitting_model=fitting_model,\n",
    "                    fit_status=\"ok\",\n",
    "                    root_status=root_status,\n",
    "                    n_required=int(round(n_req)),\n",
    "                    delta_n=int(round(max(0.0, n_req - n_current_max))),\n",
    "                    ci95_n_lo=int(np.round(ci_lo_n)),\n",
    "                    ci95_n_hi=int(np.round(ci_hi_n)),\n",
    "                    new_samples_req=int(samples_req),\n",
    "                    new_samples_req_lo=int(np.ceil(lo_samples)),\n",
    "                    new_samples_req_hi=int(np.ceil(hi_samples))\n",
    "                )\n",
    "\n",
    "            rows.append(row)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only bootstrap CI mode is currently supported.\")\n",
    "\n",
    "    # ---- Final DataFrame ----\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Move sample-related columns to the end\n",
    "    sample_cols = [\n",
    "        \"n_required\", \"delta_n\",\n",
    "        \"ci95_n_lo\", \"ci95_n_hi\", \"new_samples_req\",\n",
    "        \"new_samples_req_lo\", \"new_samples_req_hi\"\n",
    "    ]\n",
    "    all_cols = [col for col in df.columns if col not in sample_cols] + sample_cols\n",
    "    df = df[all_cols]\n",
    "\n",
    "    if save_path is not None:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Dataframe saved as .csv at: {save_path}\")\n",
    "\n",
    "    return df, pred_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e5c8c-cee2-45ad-9b70-d8b3fe29d280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /home/fs1620/MLBD_2024_25/Research_Project/LiaDataAnalysis/SampleSizePowerCalc/TunedModels_config1/svm_v4_pipeline_config1.pkl\n",
      "\n",
      "Training fraction = 0.05 ≈ 153 samples\n",
      "Training fraction = 0.10 ≈ 306 samples\n",
      "Training fraction = 0.15 ≈ 459 samples\n",
      "Training fraction = 0.20 ≈ 613 samples\n",
      "Training fraction = 0.25 ≈ 766 samples\n",
      "Training fraction = 0.30 ≈ 919 samples\n",
      "Training fraction = 0.35 ≈ 1072 samples\n",
      "Training fraction = 0.40 ≈ 1226 samples\n",
      "Training fraction = 0.45 ≈ 1379 samples\n",
      "Training fraction = 0.50 ≈ 1532 samples\n",
      "Training fraction = 0.55 ≈ 1685 samples\n",
      "Training fraction = 0.60 ≈ 1839 samples\n",
      "Training fraction = 0.65 ≈ 1992 samples\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_file = model_dir / f'svm_v4_pipeline_{config}.pkl'\n",
    "model = joblib.load(model_file)\n",
    "print(f\"Loaded model from {model_file}\\n\")\n",
    "model_name = 'SVM'\n",
    "\n",
    "overall_metrics = ['sensitivity', 'precision', 'specificity', \n",
    "                   'f1_score', 'gmean']\n",
    "core_metrics = ['sensitivity', 'precision', 'specificity']\n",
    "combined_metrics = ['f1_score', 'gmean']\n",
    "\n",
    "results_svm = cv_bootstrap_scores(\n",
    "    X=X, y=y,\n",
    "    model=model,\n",
    "    metrics=overall_metrics,\n",
    "    sample_fracs=np.linspace(0.05, 1, 20),\n",
    "    n_bootstrap = 50,\n",
    "    replace = True,\n",
    "    n_splits = 5,\n",
    "    n_jobs = -1,\n",
    "    save_results=True,\n",
    "    save_path=f'BootstrapResults_config1/learning_curves_svm_{config}.pkl'\n",
    ")\n",
    "\n",
    "results_svm = joblib.load(bootstrap_results_dir/f'learning_curves_svm_{config}.pkl')\n",
    "\n",
    "# print('\\n')\n",
    "\n",
    "fig_std, axs_std = plot_learning_curves(results=results_svm, \n",
    "                                        core_metrics=core_metrics, combined_metrics=combined_metrics,\n",
    "                                        plot_ci=True, ci_type = 'std',\n",
    "                                        save_plot=True, save_folder='LearningCurves_config1',\n",
    "                                        title_prefix = f\"Learning Curves ±1 Std. Dev., Model: {model_name}\",\n",
    "                                        save_name=f'LearningCurves_{model_name}_std.png')\n",
    "\n",
    "fig_95, axs_95 = plot_learning_curves(results=results_svm, \n",
    "                                      core_metrics=core_metrics, combined_metrics=combined_metrics,\n",
    "                                      plot_ci=True, ci_type = 'ci95',\n",
    "                                      save_plot=True, save_folder='LearningCurves_config1',\n",
    "                                      title_prefix = f\"Learning Curves 95% Bootstrap CI, Model: {model_name}\",\n",
    "                                      save_name=f'LearningCurves_{model_name}_ci_95.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d8fba-c3a9-4bc7-aa12-36f93115be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ROI Mask generation, found expected annotated number of pixels per sample = 183\n",
    "# store in expected_annotated_per_sample = 183, round down to 180 for conservative count\n",
    "# hardcoded for simplicity\n",
    "\n",
    "expected_annotated_per_sample = 180\n",
    "\n",
    "total_n_current = len(X)\n",
    "metrics_to_plan = [\"precision\", \"sensitivity\", \"specificity\", \"gmean\", \"f1_score\"]\n",
    "\n",
    "# 1.02 multiplicative increase over current baseline (max value of metric), for all metrics\n",
    "improvements_dict = {\"precision\": (\"mult\", 1.02), \n",
    "                     \"sensitivity\": (\"mult\", 1.02), \n",
    "                     \"specificity\": (\"mult\", 1.02), \n",
    "                     \"gmean\": (\"mult\", 1.02), \n",
    "                     \"f1_score\": (\"mult\", 1.01)}\n",
    "\n",
    "# Use the same \"logit_logn\" fitting model for all metrics\n",
    "fitting_models_dict = {\"precision\": \"logit_logn\", \n",
    "                     \"sensitivity\": \"logit_logn\",\n",
    "                     \"specificity\": \"logit_logn\", \n",
    "                     \"gmean\": \"logit_logn\", \n",
    "                     \"f1_score\": \"logit_logn\"}\n",
    "\n",
    "model_save_name = \"logit_logn\"\n",
    "\n",
    "df_plan, pred_fns = plan_required_samples(\n",
    "    results=results_svm,\n",
    "    metrics=metrics_to_plan,\n",
    "    total_n_current=len(X),\n",
    "    improvement=improvements_dict,\n",
    "    model=fitting_models_dict,\n",
    "    ci_mode=\"bootstrap\",\n",
    "    expected_annotated_per_sample = expected_annotated_per_sample,\n",
    "    save_path=f\"LearningCurves_config1/LCAnalysis_{model_save_name}_svm.csv\",\n",
    ")\n",
    "\n",
    "display(df_plan)\n",
    "\n",
    "# plot_learning_curves(\n",
    "#     results,\n",
    "#     core_metrics=core_metrics,\n",
    "#     combined_metrics=combined_metrics,\n",
    "#     overlay_extrapolation=True,\n",
    "#     extrapolation_df=df_plan,\n",
    "#     pred_fns=pred_fns,\n",
    "#     total_n_current=len(X)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3fc83e-2ece-4c36-99cd-997ec9140253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
